import marimo

__generated_with = "0.19.11"
app = marimo.App()


@app.cell
def _():
    import marimo as mo

    return (mo,)


@app.cell
def _():
    import numpy as np
    import plotly.graph_objects as go

    return go, np


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    At its core, simple linear regression assumes that the data is generated by a process like:

    $$
    Y = \beta_0 + \beta_1 X_1 + \epsilon
    $$

    Where:

    - **$Y$** is the **target variable** (the value we want to predict/infer).
    - **$X_1$** is the **predictor variable** (the input feature).
    - **$\beta_0$** is the **intercept**, representing the baseline value of $Y$ when $X_1 = 0$.
    - **$\beta_1$** is the **coefficient**, representing the effect of $X_1$ on $Y$ (how much $Y$ changes for a one-unit change in $X_1$).
    - **$\epsilon$** is the **error term**, capturing the variability not explained by the model.
    """)
    return


@app.cell
def _(mo):
    ui_epsilon_std_dev = mo.ui.slider(0, 5, 0.5, 1)
    return (ui_epsilon_std_dev,)


@app.cell
def _(mo, ui_epsilon_std_dev):
    mo.md(f"""
        Slide to adjust the standard deviation of epsilon $\\epsilon$: {ui_epsilon_std_dev}

        The current value is: {ui_epsilon_std_dev.value}
    """)
    return


@app.cell
def _(np, ui_epsilon_std_dev):
    np.random.seed(42)
    X = np.random.uniform(0, 10, 100)
    _epsilon = np.random.normal(0, ui_epsilon_std_dev.value, 100)  # 100 random values between 0 and 10
    Y = 2 * X + 1 + _epsilon
    return X, Y


@app.cell
def _(X, Y, go, mo):
    _fig = go.Figure()
    _fig.add_trace(
        go.Scatter(
            x=X,
            y=Y,
            mode="markers",
            name="Population Data",
            marker={"color": "blue", "opacity": 0.6},
        )
    )
    _fig.update_layout(
        title="Synthetic Data: Y = 2X + 1 + Normal Error",
        xaxis_title="X",
        yaxis_title="Y",
    )
    mo.ui.plotly(_fig)
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    So we produce a model for the data like:

    $$
    \hat{y} =  \beta_0 + \beta_1x
    $$

    The value that linear regression aims to minimise is the residual sum of squares

    $$
    RSS = \sum_i^n (y_i- \hat{y_i})^2
    $$

    Where
    - $y_i$ is our known sample value at data point $i$
    - $\hat{y_i}$ is the regression value at data point $i$

    The optimal estimates for $\beta_0$ and $\beta_1$ are as follows:

    * $\hat{\beta}_1 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}$
    * $\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$
    """)
    return


@app.cell
def _(X, Y, np):
    # IRL we don't have a population, we get a sample of observed data
    sample_indices = np.random.choice(len(X), size=50, replace=False)
    x_sample = X[sample_indices]
    y_sample = Y[sample_indices]

    x_mean = np.mean(X)
    y_mean = np.mean(Y)

    optimal_beta_1 = np.sum((x_sample - x_mean) * (y_sample - y_mean)) / np.sum((x_sample - x_mean) ** 2)

    optimal_beta_0 = y_mean - (optimal_beta_1 * x_mean)
    return optimal_beta_0, optimal_beta_1, x_sample, y_mean, y_sample


@app.cell
def _(mo):
    ui_reset_button = mo.ui.button(label="Reset $\\hat{{\\beta}}$ values")
    return (ui_reset_button,)


@app.cell
def _(mo, optimal_beta_0, optimal_beta_1, ui_reset_button):
    ui_reset_button.value
    ui_beta_1 = mo.ui.slider(optimal_beta_1 - 1, optimal_beta_1 + 1, 0.25, optimal_beta_1)
    ui_beta_0 = mo.ui.slider(optimal_beta_0 - 1, optimal_beta_0 + 1, 0.25, optimal_beta_0)
    return ui_beta_0, ui_beta_1


@app.cell
def _(mo, ui_beta_0, ui_beta_1, ui_reset_button):
    mo.md(f"""
        Currently, $\\hat{{\\beta}}_1$ and $\\hat{{\\beta}}_0$ are set at their optimal values.
        Try adjusting them and notice that any other value gives a higher $RSS$.

        Adjust $\\hat{{\\beta}}_1$: {ui_beta_1}
        $\\hat{{\\beta}}_1 = {ui_beta_1.value}$

        Adjust $\\hat{{\\beta}}_0$: {ui_beta_0}
        $\\hat{{\\beta}}_0 = {ui_beta_0.value}$

        {ui_reset_button}
    """)
    return


@app.cell
def _(ui_beta_0, ui_beta_1):
    beta_1 = ui_beta_1.value
    beta_0 = ui_beta_0.value
    return beta_0, beta_1


@app.cell
def _(X, Y, beta_0, beta_1, go, mo, np, x_sample, y_sample):
    x_domain = np.linspace(0, 10, 100)

    x_domain = np.linspace(0, 10, 100)

    y_hat = (x_domain * beta_1) + beta_0

    y_hat_known = (x_sample * beta_1) + beta_0

    residuals = y_sample - y_hat_known

    _fig = go.Figure()
    _fig.add_trace(
        go.Scatter(
            x=X,
            y=Y,
            mode="markers",
            name="Population Data",
            marker={"color": "blue", "opacity": 0.6},
        )
    )
    _fig.add_trace(
        go.Scatter(
            x=x_domain,
            y=y_hat,
            mode="lines",
            name="Sample Regression Line",
            line={"color": "red"},
        )
    )
    for i in range(len(x_sample)):
        _fig.add_trace(
            go.Scatter(
                x=[x_sample[i], x_sample[i]],
                y=[y_sample[i], y_hat_known[i]],
                mode="lines",
                showlegend=False,
                line={"color": "black", "dash": "dot"},
                opacity=0.7,
                hoverinfo="skip",
            )
        )
    _fig.update_layout(
        title="Synthetic Data: Y = 2X + 1 + Normal Error",
        xaxis_title="X",
        yaxis_title="Y",
    )
    mo.ui.plotly(_fig)
    return (y_hat_known,)


@app.cell
def _(beta_0, beta_1):
    print("beta_0 ", beta_0)
    print("beta_1 ", beta_1)
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    Something interesting, however, is that even if we did know the 'perfect' coefficients we'd still have an error value. So, the residuals from our model are technically estimates of the 'true' error terms $\epsilon$
    """)
    return


@app.cell
def _(np, y_hat_known, y_sample):
    rss = np.sum((y_hat_known - y_sample)**2)
    print(rss)
    return (rss,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    The residual standard error is an estimate of how much our regression will deviate from the true line. It is a measure of goodness of fit:

    $$
    RSE = \sqrt{\frac{1}{n-2}RSS}
    $$

    The $-2$ term is an artefact produced by the fact that we are using estimation.
    """)
    return


@app.cell
def _(np, rss, x_sample):
    rse = np.sqrt((1/(len(x_sample) - 2) )* rss)
    print(rse)
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    Now, RSE is great, but it is quoted in the units of the Y variable, so it is hard to judge how big is 'too big'. What would be great is if we had a more definitive and easy to interpret measure... This is where the $R^2$ statistic comes in:

    $$
    R^2 = \frac{TSS - RSS}{TSS}
    $$

    Where:
    - $TSS = \sum{(y_i - \bar{y})^2}$ is the 'total sum of squares'
    - $RSS$ is the residual sum of squares

    Who cares about $TSS$? Well, it measures the total variance in $y$ - it's basically how much variance exists in the target variable before any regression is done. So what we're measuring is the variance minus the deviation of our model from the data ($RSS$) divided by the total variance.

    **Thus $R^2$ is the total proportion of the variance explained by the regression!**

    If $R^2$ is equal to 0 this means that none of the variance is explained by the regression, but if $R^2$ is 1 it means all of the variance is explained by the regression.
    """)
    return


@app.cell
def _(np, rss, y_mean, y_sample):
    tss = np.sum((y_sample - y_mean)**2)

    r_squared = (tss - rss)/tss

    print(r_squared)
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    ## Multiple Linear Regression

    Multiple linear regression is an extension of simple linear regression by adding more predictor variables. So we assume that our data is generated by a process like:

    $$
    Y = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_px_p + \epsilon
    $$

    The model we use is no longer a regression 'line' but rather a regression plane with p dimensions.
    """)
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    We can estimate the regression coefficients $\bold{\beta}$ using the **Normal Equation**:

    $$
    \beta = (X^T X)^{-1} X^T Y
    $$

    Where:
    - $X$ is the **design matrix**, containing the predictor variables and a column of ones for the intercept.
    - $Y$ is the **target variable vector**.
    - $\beta$ is the **coefficient vector**, which we aim to compute.

    For example multiple regression with two predictors:

    $$
    Y = \beta X + \epsilon
    $$

    The **design matrix** $ X $ includes a column of ones for the intercept term:

    $$
    X =
    \begin{bmatrix}
    1 & X_{1,1} & X_{1,2} \\
    1 & X_{2,1} & X_{2,2} \\
    1 & X_{3,1} & X_{3,2} \\
    \vdots & \vdots & \vdots \\
    1 & X_{n,1} & X_{n,2}
    \end{bmatrix}
    $$

    where:
    - $n$ is the number of observations.
    - $X_{i,1}$ and $X_{i,2}$ are the values of the two predictor variables for each observation.
    """)
    return


@app.cell
def _(go, mo, np):
    X1 = np.random.uniform(0, 10, 100)
    X2 = np.random.uniform(0, 10, 100)
    _epsilon = np.random.normal(0, 5, 100)
    Y_1 = 1 + 2 * X1 + 3 * X2 + _epsilon
    X_1 = np.column_stack((np.ones(len(X1)), X1, X2))
    beta_hat = np.linalg.inv(X_1.T @ X_1) @ X_1.T @ Y_1
    beta_0_1, beta_1_1, beta_2 = beta_hat
    print(f'Manually Computed Coefficients:')
    print(f'Intercept (beta_0): {beta_0_1:.3f}')
    print(f'Coefficient for X1 (beta_1): {beta_1_1:.3f}')
    print(f'Coefficient for X2 (beta_2): {beta_2:.3f}')
    X1_grid, X2_grid = np.meshgrid(np.linspace(0, 10, 10), np.linspace(0, 10, 10))
    Y_grid = beta_0_1 + beta_1_1 * X1_grid + beta_2 * X2_grid
    _fig = go.Figure()
    _fig.add_trace(
        go.Scatter3d(
            x=X1,
            y=X2,
            z=Y_1,
            mode="markers",
            name="Observed Data",
            marker={"size": 4, "color": "blue", "opacity": 0.6},
        )
    )
    _fig.add_trace(
        go.Surface(
            x=X1_grid,
            y=X2_grid,
            z=Y_grid,
            name="Regression Plane",
            opacity=0.5,
            showscale=False,
        )
    )
    _fig.update_layout(
        title="Multiple Linear Regression Plane",
        scene={"xaxis_title": "X1", "yaxis_title": "X2", "zaxis_title": "Y"},
    )
    mo.ui.plotly(_fig)
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    Defintions for $RSS$, $RSE$, $TSS$ and $R^2$ extend naturally to the multiple linear regression case
    """)
    return


if __name__ == "__main__":
    app.run()
